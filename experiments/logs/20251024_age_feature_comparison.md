# 年齢特徴量の比較実験

**日付**: 2025-10-24
**目的**: 年齢特徴量の異なる使い方で精度がどう変わるかを調査

## 仮説

年齢特徴量には以下の3つの使い方があり、それぞれ精度が異なる可能性がある：
1. 年齢の生値をそのまま使用
2. 年齢層（Child, Adult, Elderly）としてラベル化して使用
3. 両方を併用

ドメイン知識に基づいた年齢層の区分が、モデルの精度向上に寄与するかを検証する。

## 実験内容

### データ
- 使用データ: train.csv (891行, 12列)
- ターゲット: Survived (生存フラグ)

### 年齢層の定義
ドメイン知識に基づいて以下のように区分：
```python
bins_domain_knowledge = [0, 18, 60, 80]
labels_domain_knowledge = ['Child', 'Adult', 'Elderly']
```

- **Child (0-18歳)**: 子供。優先的に救助される可能性
- **Adult (18-60歳)**: 成人。一般的な乗客
- **Elderly (60-80歳)**: 高齢者。救助が困難な可能性

### 特徴量

**基本特徴量（全ケースで共通）:**
- Pclass: チケットクラス
- Sex: 性別（0=male, 1=female）
- Title: 敬称（Mr, Miss, Mrs, Master, Rare）
- IsAlone: 一人旅フラグ
- FareBin: 運賃のビン化（4分位）
- SmallFamily: 小規模家族フラグ（2-4人）
- Age_Missing: 年齢欠損値フラグ
- Embarked_Missing: 乗船港欠損値フラグ

**年齢関連の特徴量（ケース別）:**
- ケース1: Age（年齢の生値）
- ケース2: AgeGroup_DomainKnowledge（年齢層ラベル）
- ケース3: Age + AgeGroup_DomainKnowledge（両方）

### モデル

**LightGBM:**
```python
LGBMClassifier(
    n_estimators=100,
    max_depth=2,
    num_leaves=4,
    min_child_samples=30,
    reg_alpha=1.0,
    reg_lambda=1.0,
    min_split_gain=0.01,
    learning_rate=0.05,
    random_state=42
)
```

**Random Forest:**
```python
RandomForestClassifier(
    n_estimators=100,
    max_depth=5,
    min_samples_split=10,
    min_samples_leaf=5,
    random_state=42
)
```

### 評価方法
- クロスバリデーション: 5-Fold Stratified K-Fold
- 評価指標: Accuracy
- 乱数シード: 42

## 結果

### LightGBM

| ケース           | 特徴量数 | CV Score | CV Std   | Train Score | Gap      | CV変化     |
|------------------|----------|----------|----------|-------------|----------|------------|
| 年齢の生値のみ   | 9        | 0.82716  | 0.01300  | 0.84400     | 0.01683  | 0.00000    |
| 年齢層ラベルのみ | 9        | 0.82940  | 0.00581  | 0.83614     | 0.00674  | +0.00224   |
| 両方を使用       | 10       | 0.82716  | 0.01300  | 0.84400     | 0.01683  | 0.00000    |

**最良のケース**: 年齢層ラベルのみ（CV Score: 0.82940）

**相互作用の評価:**
- 年齢層ラベルのみの改善: +0.00224
- 両方使用時の改善: +0.00000
- 相互作用効果: -0.00224
- **→ 負の相互作用あり**（特徴量が互いに干渉している）

### Random Forest

| ケース           | 特徴量数 | CV Score | CV Std   | Train Score | Gap      | CV変化     |
|------------------|----------|----------|----------|-------------|----------|------------|
| 年齢の生値のみ   | 9        | 0.83391  | 0.01134  | 0.84400     | 0.01009  | 0.00000    |
| 年齢層ラベルのみ | 9        | 0.82828  | 0.00771  | 0.84287     | 0.01459  | -0.00563   |
| 両方を使用       | 10       | 0.83390  | 0.01258  | 0.84736     | 0.01347  | -0.00001   |

**最良のケース**: 年齢の生値のみ（CV Score: 0.83391）

**相互作用の評価:**
- 年齢層ラベルのみの改善: -0.00563
- 両方使用時の改善: -0.00001
- 相互作用効果: +0.00562
- **→ 正の相互作用あり**（特徴量が相乗効果を発揮）

## 考察

### モデル別の傾向

**LightGBM:**
- 年齢層ラベルのみが最も良い結果（CV: 0.82940）
- 年齢層ラベルはCV標準偏差が小さい（0.00581）→ 安定性が高い
- 両方を使用すると、年齢の生値のみと同じスコアに戻る（負の相互作用）
- LightGBMは浅い木（max_depth=2）のため、カテゴリカルな区分が効果的

**Random Forest:**
- 年齢の生値のみが最も良い結果（CV: 0.83391）
- 年齢層ラベルのみは精度が低下（-0.00563）
- 両方を使用すると、年齢の生値のみとほぼ同じスコア（わずかに低下）
- Random Forestは深い木（max_depth=5）のため、連続値のまま使う方が効果的

### うまくいった点

1. **LightGBMでの年齢層ラベルの効果**:
   - 年齢層ラベルのみで最も安定した結果（CV Std: 0.00581）
   - 過学習が減少（Gap: 0.00674）
   - ドメイン知識に基づいた区分が、浅い決定木モデルに適合

2. **Random Forestでの年齢生値の優位性**:
   - 連続値のまま使用することで細かい境界を学習
   - 深い木構造が年齢の非線形関係を捉えられる

### うまくいかなかった点

1. **両方を使用したケース**:
   - LightGBMで負の相互作用（-0.00224）
   - Random Forestでもほとんど改善なし（-0.00001）
   - 年齢と年齢層が情報的に冗長で、モデルを複雑化

2. **モデル間の不一致**:
   - LightGBMとRandom Forestで最適な特徴量が異なる
   - モデルアーキテクチャに応じた特徴量設計が必要

### 学んだこと

1. **モデルの性質に応じた特徴量選択**:
   - 浅い木モデル（LightGBM）→ カテゴリカル変換が有効
   - 深い木モデル（Random Forest）→ 連続値のまま使う方が有効

2. **年齢層の区分の効果**:
   - ドメイン知識に基づいた年齢層区分は、一定の効果がある
   - 特にLightGBMで安定性が向上（CV Stdが半減）

3. **特徴量の冗長性**:
   - 関連性の高い特徴量を同時に使用すると、負の相互作用が発生
   - 情報量を増やすより、モデルに適した表現を選ぶことが重要

4. **過学習の観点**:
   - 年齢層ラベルは過学習を抑制（LightGBM: Gap 0.00674 vs 0.01683）
   - カテゴリカル変換による情報の粗さが正則化効果を持つ

## 次のアクション

- [ ] LightGBMで年齢層ラベルを採用し、他の特徴量との組み合わせを検証
- [ ] Random Forestでは年齢の生値を維持
- [ ] 他のドメイン知識に基づいた特徴量変換を試す（例: FareGroup, TitleGroup）
- [ ] アンサンブルモデルで両方の特徴量表現を活用できるか検証

## 関連ファイル

- スクリプト: `scripts/feature_engineering/age_feature_comparison.py`
- 実験ログ: `experiments/logs/20251024_age_feature_comparison.md`
- 実験結果: コンソール出力（上記に記載）
