================================================================================
年齢特徴量の比較実験 - 結果サマリー
================================================================================
日付: 2025-10-24
実験者: Claude Code

================================================================================
実験概要
================================================================================
年齢特徴量の3つの使い方を比較：
  1. 年齢の生値のみ（Age）
  2. 年齢層ラベルのみ（AgeGroup_DomainKnowledge: Child/Adult/Elderly）
  3. 両方を使用（Age + AgeGroup_DomainKnowledge）

年齢層の定義:
  - Child: 0-18歳
  - Adult: 18-60歳
  - Elderly: 60-80歳

モデル: LightGBM, Random Forest
評価: 5-Fold Stratified K-Fold Cross Validation

================================================================================
結果サマリー
================================================================================

【LightGBM】
------------------------------------------------
ケース              | CV Score | CV Std  | Gap    | 順位
------------------------------------------------
年齢の生値のみ      | 0.82716  | 0.01300 | 0.01683 | 2位
年齢層ラベルのみ    | 0.82940  | 0.00581 | 0.00674 | 1位 ★
両方を使用          | 0.82716  | 0.01300 | 0.01683 | 2位
------------------------------------------------

最良: 年齢層ラベルのみ
改善: +0.00224 (年齢の生値のみと比較)
相互作用: 負（-0.00224）

【Random Forest】
------------------------------------------------
ケース              | CV Score | CV Std  | Gap    | 順位
------------------------------------------------
年齢の生値のみ      | 0.83391  | 0.01134 | 0.01009 | 1位 ★
年齢層ラベルのみ    | 0.82828  | 0.00771 | 0.01459 | 3位
両方を使用          | 0.83390  | 0.01258 | 0.01347 | 2位
------------------------------------------------

最良: 年齢の生値のみ
改善: 年齢層ラベルは-0.00563の低下
相互作用: 正（+0.00562）

================================================================================
重要な発見
================================================================================

1. モデル依存性:
   - LightGBM: 年齢層ラベルが最適（CV: 0.82940）
   - Random Forest: 年齢の生値が最適（CV: 0.83391）
   → モデルアーキテクチャに応じた特徴量選択が重要

2. 安定性の向上:
   - 年齢層ラベル（LightGBM）: CV Std 0.00581（最も安定）
   - 年齢の生値（LightGBM）: CV Std 0.01300
   → カテゴリカル変換が安定性を向上

3. 過学習の抑制:
   - 年齢層ラベル（LightGBM）: Gap 0.00674
   - 年齢の生値（LightGBM）: Gap 0.01683
   → カテゴリカル変換が正則化効果を持つ

4. 両方使用の効果:
   - LightGBM: 負の相互作用（-0.00224）
   - Random Forest: わずかな正の相互作用（+0.00562）だが改善なし
   → 情報の冗長性により効果が限定的

================================================================================
推奨事項
================================================================================

1. LightGBM使用時:
   - 年齢層ラベル（AgeGroup_DomainKnowledge）を採用
   - 理由: 最高精度、安定性、過学習抑制

2. Random Forest使用時:
   - 年齢の生値（Age）を採用
   - 理由: 最高精度、連続値の細かい境界を学習

3. アンサンブル戦略:
   - LightGBMとRandom Forestで異なる特徴量を使用
   - 多様性の向上により、アンサンブル効果が期待できる

4. 今後の検証:
   - 他のカテゴリカル変換（FareGroup, TitleGroupなど）
   - アンサンブルモデルでの両方の特徴量表現の活用

================================================================================
詳細ログ
================================================================================
experiments/logs/20251024_age_feature_comparison.md

================================================================================
