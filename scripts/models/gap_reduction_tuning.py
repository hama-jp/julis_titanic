"""
Gap Reduction Tuning - éå­¦ç¿’å‰Šæ¸›ã«ç‰¹åŒ–ã—ãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

ç›®çš„: CV Scoreã¨Train Scoreã®å·®ï¼ˆGapï¼‰ã‚’æœ€å°åŒ–
ç¾çŠ¶:
  - LightGBM: Gap 0.0090
  - RandomForest: Gap 0.0112
  - GradientBoosting: Gap 0.0314
ç›®æ¨™: Gap < 0.005 ã‚’ç›®æŒ‡ã™

æˆ¦ç•¥:
  1. æ­£å‰‡åŒ–ã®å¼·åŒ–ï¼ˆreg_alpha, reg_lambda, Cå€¤ï¼‰
  2. ãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘åº¦å‰Šæ¸›ï¼ˆmax_depth, num_leavesï¼‰
  3. ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã®å¢—åŠ ï¼ˆmin_samples_leaf, min_child_samplesï¼‰
  4. Early Stopping
  5. Dropout/Subsampleèª¿æ•´
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
import lightgbm as lgb
from sklearn.preprocessing import LabelEncoder
import warnings
warnings.filterwarnings('ignore')

print('='*80)
print('GAP REDUCTION TUNING - éå­¦ç¿’å‰Šæ¸›ç‰¹åŒ–å‹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°')
print('='*80)
print('ç›®æ¨™: Gap < 0.005 ã‚’é”æˆ')
print('='*80)

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
train = pd.read_csv('data/raw/train.csv')
test = pd.read_csv('data/raw/test.csv')

# ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
def engineer_features(df):
    """9ç‰¹å¾´é‡ã®åŸºæœ¬ã‚»ãƒƒãƒˆï¼ˆDeck_Safeé™¤å¤–ï¼‰"""
    df = df.copy()

    # Missing flags BEFORE imputation
    df['Age_Missing'] = df['Age'].isnull().astype(int)
    df['Embarked_Missing'] = df['Embarked'].isnull().astype(int)

    # Title extraction
    df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\.', expand=False)
    title_mapping = {
        'Mr': 'Mr', 'Miss': 'Miss', 'Mrs': 'Mrs', 'Master': 'Master',
        'Rev': 'Rare', 'Dr': 'Rare', 'Col': 'Rare', 'Major': 'Rare',
        'Mlle': 'Miss', 'Mme': 'Mrs', 'Don': 'Rare', 'Dona': 'Rare',
        'Lady': 'Rare', 'Countess': 'Rare', 'Jonkheer': 'Rare',
        'Sir': 'Rare', 'Capt': 'Rare', 'Ms': 'Miss'
    }
    df['Title'] = df['Title'].map(title_mapping)

    # Family features
    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1
    df['IsAlone'] = (df['FamilySize'] == 1).astype(int)
    df['SmallFamily'] = ((df['FamilySize'] >= 2) & (df['FamilySize'] <= 4)).astype(int)

    # Age imputation by Title + Pclass
    df['Age'] = df.groupby(['Title', 'Pclass'])['Age'].transform(
        lambda x: x.fillna(x.median())
    )

    # Fare imputation and binning
    df['Fare'] = df.groupby('Pclass')['Fare'].transform(
        lambda x: x.fillna(x.median())
    )
    df['FareBin'] = pd.qcut(df['Fare'], q=4, labels=[0, 1, 2, 3], duplicates='drop')
    df['FareBin'] = df['FareBin'].astype(int)

    # Embarked
    df['Embarked'] = df['Embarked'].fillna('S')

    return df

print('\nç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°...')
train_fe = engineer_features(train)
test_fe = engineer_features(test)

# Encoding
le_title = LabelEncoder()
train_fe['Title'] = le_title.fit_transform(train_fe['Title'])
test_fe['Title'] = le_title.transform(test_fe['Title'])

# 9ç‰¹å¾´é‡ï¼ˆLB 0.77751ã§æ¤œè¨¼æ¸ˆã¿ï¼‰
features = ['Pclass', 'Sex', 'Title', 'Age', 'IsAlone', 'FareBin',
            'SmallFamily', 'Age_Missing', 'Embarked_Missing']

# Sex encoding
train_fe['Sex'] = train_fe['Sex'].map({'male': 0, 'female': 1})
test_fe['Sex'] = test_fe['Sex'].map({'male': 0, 'female': 1})

X_train = train_fe[features]
y_train = train_fe['Survived']
X_test = test_fe[features]

print(f'\nTraining data: {len(X_train)} samples')
print(f'Features ({len(features)}): {", ".join(features)}')
print(f'Test data: {len(X_test)} samples')

# Cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# è©•ä¾¡é–¢æ•°
def evaluate_gap(model, X, y, cv, model_name):
    """Gapã‚’é‡è¦–ã—ãŸè©•ä¾¡"""
    # CV score
    cv_scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')
    cv_mean = cv_scores.mean()
    cv_std = cv_scores.std()

    # Train score
    model.fit(X, y)
    train_score = model.score(X, y)

    # Gapè¨ˆç®—
    gap = train_score - cv_mean

    return {
        'model': model_name,
        'cv_mean': cv_mean,
        'cv_std': cv_std,
        'train_score': train_score,
        'gap': gap,
        'gap_percentage': gap / cv_mean * 100
    }

print('\n' + '='*80)
print('ç¾çŠ¶ã®Gapè©•ä¾¡ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰')
print('='*80)

baseline_models = {
    'RF_Current': RandomForestClassifier(
        n_estimators=100,
        max_depth=4,
        min_samples_split=20,
        min_samples_leaf=10,
        max_features='sqrt',
        random_state=42
    ),
    'GB_Current': GradientBoostingClassifier(
        n_estimators=100,
        max_depth=3,
        learning_rate=0.05,
        min_samples_split=20,
        min_samples_leaf=10,
        subsample=0.8,
        random_state=42
    ),
    'LGB_Current': lgb.LGBMClassifier(
        n_estimators=100,
        max_depth=2,
        num_leaves=4,
        min_child_samples=30,
        reg_alpha=1.0,
        reg_lambda=1.0,
        min_split_gain=0.01,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        verbose=-1
    )
}

baseline_results = []
for name, model in baseline_models.items():
    result = evaluate_gap(model, X_train, y_train, cv, name)
    baseline_results.append(result)
    print(f"\n{name}:")
    print(f"  CV Score: {result['cv_mean']:.4f} (Â±{result['cv_std']:.4f})")
    print(f"  Train Score: {result['train_score']:.4f}")
    print(f"  Gap: {result['gap']:.4f} ({result['gap_percentage']:.2f}%)")

# ========================================================================
# Gapå‰Šæ¸›ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
# ========================================================================

print('\n' + '='*80)
print('Gapå‰Šæ¸›ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹')
print('='*80)

tuned_results = []

# ========================================================================
# 1. Random Forest - è¶…ä¿å®ˆçš„è¨­å®š
# ========================================================================
print('\nã€1. Random Forest - è¶…ä¿å®ˆçš„è¨­å®šã€‘')
print('æˆ¦ç•¥: max_depthå‰Šæ¸›ã€min_samples_leafå¢—åŠ ')

rf_configs = [
    {
        'name': 'RF_UltraConservative_D3',
        'params': {
            'n_estimators': 100,
            'max_depth': 3,  # 4 â†’ 3
            'min_samples_split': 30,  # 20 â†’ 30
            'min_samples_leaf': 15,  # 10 â†’ 15
            'max_features': 'sqrt',
            'random_state': 42
        }
    },
    {
        'name': 'RF_UltraConservative_D2',
        'params': {
            'n_estimators': 100,
            'max_depth': 2,  # ã•ã‚‰ã«å‰Šæ¸›
            'min_samples_split': 40,
            'min_samples_leaf': 20,
            'max_features': 'sqrt',
            'random_state': 42
        }
    },
    {
        'name': 'RF_MinComplexity',
        'params': {
            'n_estimators': 50,  # æœ¨ã®æ•°ã‚‚å‰Šæ¸›
            'max_depth': 3,
            'min_samples_split': 25,
            'min_samples_leaf': 12,
            'max_features': 'sqrt',
            'min_impurity_decrease': 0.001,  # åˆ†å²ã®æœ€å°æ”¹å–„
            'random_state': 42
        }
    }
]

for config in rf_configs:
    model = RandomForestClassifier(**config['params'])
    result = evaluate_gap(model, X_train, y_train, cv, config['name'])
    tuned_results.append(result)
    print(f"\n{config['name']}:")
    print(f"  CV: {result['cv_mean']:.4f}, Gap: {result['gap']:.4f} ({result['gap_percentage']:.2f}%)")

# ========================================================================
# 2. Gradient Boosting - å¼·åŠ›ãªæ­£å‰‡åŒ–
# ========================================================================
print('\nã€2. Gradient Boosting - å¼·åŠ›ãªæ­£å‰‡åŒ–ã€‘')
print('æˆ¦ç•¥: learning_rateå‰Šæ¸›ã€max_depthå‰Šæ¸›ã€subsampleå‰Šæ¸›')

gb_configs = [
    {
        'name': 'GB_StrongReg_V1',
        'params': {
            'n_estimators': 100,
            'max_depth': 2,  # 3 â†’ 2
            'learning_rate': 0.03,  # 0.05 â†’ 0.03
            'min_samples_split': 30,  # 20 â†’ 30
            'min_samples_leaf': 15,  # 10 â†’ 15
            'subsample': 0.7,  # 0.8 â†’ 0.7
            'max_features': 'sqrt',
            'random_state': 42
        }
    },
    {
        'name': 'GB_StrongReg_V2',
        'params': {
            'n_estimators': 150,  # æ•°ã‚’å¢—ã‚„ã—ã¦å­¦ç¿’ç‡ã‚’ä¸‹ã’ã‚‹
            'max_depth': 2,
            'learning_rate': 0.02,  # ã•ã‚‰ã«å‰Šæ¸›
            'min_samples_split': 35,
            'min_samples_leaf': 20,
            'subsample': 0.6,  # ã•ã‚‰ã«å‰Šæ¸›
            'max_features': 0.7,
            'random_state': 42
        }
    },
    {
        'name': 'GB_MinDepth',
        'params': {
            'n_estimators': 100,
            'max_depth': 1,  # æœ€å°æ·±åº¦
            'learning_rate': 0.05,
            'min_samples_split': 20,
            'min_samples_leaf': 10,
            'subsample': 0.8,
            'random_state': 42
        }
    }
]

for config in gb_configs:
    model = GradientBoostingClassifier(**config['params'])
    result = evaluate_gap(model, X_train, y_train, cv, config['name'])
    tuned_results.append(result)
    print(f"\n{config['name']}:")
    print(f"  CV: {result['cv_mean']:.4f}, Gap: {result['gap']:.4f} ({result['gap_percentage']:.2f}%)")

# ========================================================================
# 3. LightGBM - æœ€å¼·æ­£å‰‡åŒ– + Dropout
# ========================================================================
print('\nã€3. LightGBM - æœ€å¼·æ­£å‰‡åŒ– + Dropoutã€‘')
print('æˆ¦ç•¥: reg_alpha/lambdaå¢—åŠ ã€dropoutè¿½åŠ ã€min_child_sampleså¢—åŠ ')

lgb_configs = [
    {
        'name': 'LGB_StrongReg_V1',
        'params': {
            'n_estimators': 100,
            'max_depth': 2,
            'num_leaves': 4,
            'min_child_samples': 40,  # 30 â†’ 40
            'reg_alpha': 2.0,  # 1.0 â†’ 2.0
            'reg_lambda': 2.0,  # 1.0 â†’ 2.0
            'min_split_gain': 0.02,  # 0.01 â†’ 0.02
            'learning_rate': 0.05,
            'subsample': 0.7,  # 0.8 â†’ 0.7
            'colsample_bytree': 0.7,  # 0.8 â†’ 0.7
            'random_state': 42,
            'verbose': -1
        }
    },
    {
        'name': 'LGB_StrongReg_V2_Dropout',
        'params': {
            'n_estimators': 100,
            'max_depth': 2,
            'num_leaves': 4,
            'min_child_samples': 50,  # ã•ã‚‰ã«å¢—åŠ 
            'reg_alpha': 3.0,  # ã•ã‚‰ã«å¢—åŠ 
            'reg_lambda': 3.0,
            'min_split_gain': 0.03,
            'learning_rate': 0.05,
            'subsample': 0.6,
            'colsample_bytree': 0.6,
            'drop_rate': 0.1,  # Dropoutè¿½åŠ 
            'random_state': 42,
            'verbose': -1
        }
    },
    {
        'name': 'LGB_MinLeaves',
        'params': {
            'n_estimators': 100,
            'max_depth': 2,
            'num_leaves': 3,  # 4 â†’ 3ï¼ˆæœ€å°åŒ–ï¼‰
            'min_child_samples': 35,
            'reg_alpha': 1.5,
            'reg_lambda': 1.5,
            'min_split_gain': 0.015,
            'learning_rate': 0.05,
            'subsample': 0.75,
            'colsample_bytree': 0.75,
            'random_state': 42,
            'verbose': -1
        }
    }
]

for config in lgb_configs:
    model = lgb.LGBMClassifier(**config['params'])
    result = evaluate_gap(model, X_train, y_train, cv, config['name'])
    tuned_results.append(result)
    print(f"\n{config['name']}:")
    print(f"  CV: {result['cv_mean']:.4f}, Gap: {result['gap']:.4f} ({result['gap_percentage']:.2f}%)")

# ========================================================================
# 4. Logistic Regression - æœ€å¼·æ­£å‰‡åŒ–
# ========================================================================
print('\nã€4. Logistic Regression - æœ€å¼·æ­£å‰‡åŒ–ã€‘')
print('æˆ¦ç•¥: Cå€¤å‰Šæ¸›ï¼ˆæ­£å‰‡åŒ–å¼·åŒ–ï¼‰')

lr_configs = [
    {
        'name': 'LR_StrongReg_C01',
        'params': {
            'C': 0.1,  # 0.5 â†’ 0.1
            'penalty': 'l2',
            'max_iter': 1000,
            'random_state': 42
        }
    },
    {
        'name': 'LR_StrongReg_C005',
        'params': {
            'C': 0.05,  # ã•ã‚‰ã«å‰Šæ¸›
            'penalty': 'l2',
            'max_iter': 1000,
            'random_state': 42
        }
    },
    {
        'name': 'LR_ElasticNet',
        'params': {
            'C': 0.1,
            'penalty': 'elasticnet',
            'solver': 'saga',
            'l1_ratio': 0.5,  # L1ã¨L2ã®æ··åˆ
            'max_iter': 2000,
            'random_state': 42
        }
    }
]

for config in lr_configs:
    model = LogisticRegression(**config['params'])
    result = evaluate_gap(model, X_train, y_train, cv, config['name'])
    tuned_results.append(result)
    print(f"\n{config['name']}:")
    print(f"  CV: {result['cv_mean']:.4f}, Gap: {result['gap']:.4f} ({result['gap_percentage']:.2f}%)")

# ========================================================================
# çµæœã®ã¾ã¨ã‚
# ========================================================================

print('\n' + '='*80)
print('çµæœã‚µãƒãƒªãƒ¼')
print('='*80)

# å…¨çµæœã‚’çµåˆ
all_results = baseline_results + tuned_results
results_df = pd.DataFrame(all_results)

# Gapã§ã‚½ãƒ¼ãƒˆ
results_df_sorted = results_df.sort_values('gap')

print('\nã€Gapé †ã«ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆä¸Šä½10ï¼‰ã€‘')
print(results_df_sorted[['model', 'cv_mean', 'train_score', 'gap', 'gap_percentage']].head(10).to_string(index=False))

print('\nã€Gap < 0.005ã‚’é”æˆã—ãŸãƒ¢ãƒ‡ãƒ«ã€‘')
gap_achieved = results_df_sorted[results_df_sorted['gap'] < 0.005]
if len(gap_achieved) > 0:
    print(gap_achieved[['model', 'cv_mean', 'gap', 'gap_percentage']].to_string(index=False))
else:
    print('è©²å½“ãªã—ï¼ˆç›®æ¨™æœªé”æˆï¼‰')
    print(f'æœ€å°Gap: {results_df_sorted.iloc[0]["gap"]:.4f} ({results_df_sorted.iloc[0]["model"]})')

print('\nã€CV Scoreã‚’ç¶­æŒã—ã¤ã¤Gapã‚’å‰Šæ¸›ã—ãŸãƒ¢ãƒ‡ãƒ«ï¼ˆCV > 0.82ï¼‰ã€‘')
best_balance = results_df_sorted[(results_df_sorted['cv_mean'] > 0.82) & (results_df_sorted['gap'] < 0.01)]
if len(best_balance) > 0:
    print(best_balance[['model', 'cv_mean', 'gap']].to_string(index=False))
else:
    print('è©²å½“ãªã—')

# ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«é¸å®š
print('\n' + '='*80)
print('æ¨å¥¨ãƒ¢ãƒ‡ãƒ«é¸å®š')
print('='*80)

best_gap_model = results_df_sorted.iloc[0]
print(f"\nğŸ† æœ€å°Gapé”æˆãƒ¢ãƒ‡ãƒ«: {best_gap_model['model']}")
print(f"   CV Score: {best_gap_model['cv_mean']:.4f} (Â±{best_gap_model['cv_std']:.4f})")
print(f"   Train Score: {best_gap_model['train_score']:.4f}")
print(f"   Gap: {best_gap_model['gap']:.4f} ({best_gap_model['gap_percentage']:.2f}%)")

# CV Scoreã‚‚è€ƒæ…®ã—ãŸãƒãƒ©ãƒ³ã‚¹å‹
results_df_sorted['score'] = results_df_sorted['cv_mean'] - results_df_sorted['gap'] * 2  # Gapã«ãƒšãƒŠãƒ«ãƒ†ã‚£
best_balanced = results_df_sorted.sort_values('score', ascending=False).iloc[0]

print(f"\nâ­ ãƒãƒ©ãƒ³ã‚¹å‹ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«: {best_balanced['model']}")
print(f"   CV Score: {best_balanced['cv_mean']:.4f}")
print(f"   Gap: {best_balanced['gap']:.4f}")
print(f"   ç·åˆã‚¹ã‚³ã‚¢: {best_balanced['score']:.4f}")

# çµæœã‚’CSVã«ä¿å­˜
results_df_sorted.to_csv('experiments/results/gap_reduction_tuning_results.csv', index=False)
print(f"\nâœ… çµæœã‚’ä¿å­˜: experiments/results/gap_reduction_tuning_results.csv")

print('\n' + '='*80)
print('GAP REDUCTION TUNINGå®Œäº†ï¼')
print('='*80)
