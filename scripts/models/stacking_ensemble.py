"""
Stacking Ensemble - ãƒ¡ã‚¿å­¦ç¿’ã«ã‚ˆã‚‹é«˜ç²¾åº¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«

ç¾çŠ¶: Hard Voting OOF 0.8350
ç›®æ¨™: Stacking OOF 0.840 (+0.5%)

æˆ¦ç•¥:
1. ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«: LightGBM, GradientBoosting, RandomForest
2. ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«: LogisticRegressionï¼ˆã‚·ãƒ³ãƒ—ãƒ«ã€éå­¦ç¿’ã—ã«ãã„ï¼‰
3. Out-of-Foldäºˆæ¸¬ã§ãƒ¡ã‚¿ç‰¹å¾´é‡ç”Ÿæˆ
4. ãƒªãƒ¼ã‚±ãƒ¼ã‚¸é˜²æ­¢ï¼ˆtestãƒ‡ãƒ¼ã‚¿ã®æƒ…å ±ã¯ä½¿ã‚ãªã„ï¼‰
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold, cross_val_score, cross_val_predict
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
import lightgbm as lgb
from sklearn.metrics import accuracy_score
import warnings
warnings.filterwarnings('ignore')

print('='*80)
print('STACKING ENSEMBLE - ãƒ¡ã‚¿å­¦ç¿’ã«ã‚ˆã‚‹é«˜ç²¾åº¦ãƒ¢ãƒ‡ãƒ«')
print('='*80)
print('ç›®æ¨™: Hard Voting OOF 0.8350 â†’ Stacking OOF 0.840 (+0.5%)')
print('='*80)

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
train = pd.read_csv('data/raw/train.csv')
test = pd.read_csv('data/raw/test.csv')

# ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
def engineer_features(df):
    """9ç‰¹å¾´é‡ã®åŸºæœ¬ã‚»ãƒƒãƒˆï¼ˆDeck_Safeé™¤å¤–ã€LBæ¤œè¨¼æ¸ˆã¿ï¼‰"""
    df = df.copy()

    # Missing flags BEFORE imputation
    df['Age_Missing'] = df['Age'].isnull().astype(int)
    df['Embarked_Missing'] = df['Embarked'].isnull().astype(int)

    # Title extraction
    df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\.', expand=False)
    title_mapping = {
        'Mr': 'Mr', 'Miss': 'Miss', 'Mrs': 'Mrs', 'Master': 'Master',
        'Rev': 'Rare', 'Dr': 'Rare', 'Col': 'Rare', 'Major': 'Rare',
        'Mlle': 'Miss', 'Mme': 'Mrs', 'Don': 'Rare', 'Dona': 'Rare',
        'Lady': 'Rare', 'Countess': 'Rare', 'Jonkheer': 'Rare',
        'Sir': 'Rare', 'Capt': 'Rare', 'Ms': 'Miss'
    }
    df['Title'] = df['Title'].map(title_mapping)

    # Family features
    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1
    df['IsAlone'] = (df['FamilySize'] == 1).astype(int)
    df['SmallFamily'] = ((df['FamilySize'] >= 2) & (df['FamilySize'] <= 4)).astype(int)

    # Age imputation by Title + Pclass
    df['Age'] = df.groupby(['Title', 'Pclass'])['Age'].transform(
        lambda x: x.fillna(x.median())
    )

    # Fare imputation and binning
    df['Fare'] = df.groupby('Pclass')['Fare'].transform(
        lambda x: x.fillna(x.median())
    )
    df['FareBin'] = pd.qcut(df['Fare'], q=4, labels=[0, 1, 2, 3], duplicates='drop')
    df['FareBin'] = df['FareBin'].astype(int)

    # Embarked
    df['Embarked'] = df['Embarked'].fillna('S')

    return df

print('\nç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°...')
train_fe = engineer_features(train)
test_fe = engineer_features(test)

# Encoding
from sklearn.preprocessing import LabelEncoder
le_title = LabelEncoder()
train_fe['Title'] = le_title.fit_transform(train_fe['Title'])
test_fe['Title'] = le_title.transform(test_fe['Title'])

# 9ç‰¹å¾´é‡ï¼ˆLB 0.77751ã§æ¤œè¨¼æ¸ˆã¿ï¼‰
features = ['Pclass', 'Sex', 'Title', 'Age', 'IsAlone', 'FareBin',
            'SmallFamily', 'Age_Missing', 'Embarked_Missing']

# Sex encoding
train_fe['Sex'] = train_fe['Sex'].map({'male': 0, 'female': 1})
test_fe['Sex'] = test_fe['Sex'].map({'male': 0, 'female': 1})

X_train = train_fe[features]
y_train = train_fe['Survived']
X_test = test_fe[features]

print(f'\nTraining data: {len(X_train)} samples')
print(f'Features ({len(features)}): {", ".join(features)}')
print(f'Test data: {len(X_test)} samples')

# Cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# ========================================================================
# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«å®šç¾©ï¼ˆæ—¢å­˜ã®æœ€è‰¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
# ========================================================================
print('\n' + '='*80)
print('ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«å®šç¾©')
print('='*80)

base_models = {
    'LightGBM': lgb.LGBMClassifier(
        n_estimators=100,
        max_depth=2,
        num_leaves=4,
        min_child_samples=30,
        reg_alpha=1.0,
        reg_lambda=1.0,
        min_split_gain=0.01,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        verbose=-1
    ),
    'GradientBoosting': GradientBoostingClassifier(
        n_estimators=100,
        max_depth=3,
        learning_rate=0.05,
        min_samples_split=20,
        min_samples_leaf=10,
        subsample=0.8,
        random_state=42
    ),
    'RandomForest': RandomForestClassifier(
        n_estimators=100,
        max_depth=4,
        min_samples_split=20,
        min_samples_leaf=10,
        max_features='sqrt',
        random_state=42
    )
}

print('\n1. LightGBM (Conservative_V1)')
print('   CV Score (è¨˜éŒ²): 0.8317, Gap: 0.0090')
print('   Parameters: max_depth=2, num_leaves=4, strong regularization')

print('\n2. GradientBoosting (Conservative)')
print('   CV Score (è¨˜éŒ²): 0.8316, Gap: 0.0314')
print('   Parameters: max_depth=3, learning_rate=0.05')

print('\n3. RandomForest (Conservative)')
print('   CV Score (è¨˜éŒ²): 0.8294, Gap: 0.0090')
print('   Parameters: max_depth=4, min_samples_leaf=10')

# ========================================================================
# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®CVè©•ä¾¡ï¼ˆç¢ºèªï¼‰
# ========================================================================
print('\n' + '='*80)
print('ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®CVè©•ä¾¡ï¼ˆç¢ºèªï¼‰')
print('='*80)

base_cv_scores = {}
for name, model in base_models.items():
    cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')
    base_cv_scores[name] = {
        'mean': cv_scores.mean(),
        'std': cv_scores.std()
    }
    print(f'\n{name}:')
    print(f'  CV Score: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})')

# ========================================================================
# Hard Votingï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰
# ========================================================================
print('\n' + '='*80)
print('ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³: Hard Voting')
print('='*80)

from sklearn.ensemble import VotingClassifier

voting_hard = VotingClassifier(
    estimators=[
        ('lgb', base_models['LightGBM']),
        ('gb', base_models['GradientBoosting']),
        ('rf', base_models['RandomForest'])
    ],
    voting='hard'
)

# OOFäºˆæ¸¬ã§Hard Votingã‚’è©•ä¾¡
oof_pred_hard = cross_val_predict(voting_hard, X_train, y_train, cv=cv)
hard_oof_score = accuracy_score(y_train, oof_pred_hard)

print(f'\nHard Voting OOF Score: {hard_oof_score:.4f}')
print('ï¼ˆæœŸå¾…: 0.8350å‰å¾Œï¼‰')

# ========================================================================
# Stacking Ensemble
# ========================================================================
print('\n' + '='*80)
print('Stacking Ensemble')
print('='*80)

# ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã®å€™è£œ
meta_models = {
    'LogisticRegression_C01': LogisticRegression(C=0.1, random_state=42, max_iter=1000),
    'LogisticRegression_C05': LogisticRegression(C=0.5, random_state=42, max_iter=1000),
    'LogisticRegression_C10': LogisticRegression(C=1.0, random_state=42, max_iter=1000),
}

stacking_results = []

for meta_name, meta_model in meta_models.items():
    print(f'\nã€Stacking with {meta_name}ã€‘')

    stacking = StackingClassifier(
        estimators=[
            ('lgb', base_models['LightGBM']),
            ('gb', base_models['GradientBoosting']),
            ('rf', base_models['RandomForest'])
        ],
        final_estimator=meta_model,
        cv=5,  # å†…éƒ¨CVã§OOFäºˆæ¸¬ç”Ÿæˆ
        passthrough=False,  # å…ƒã®ç‰¹å¾´é‡ã¯ä½¿ã‚ãªã„ï¼ˆãƒ¡ã‚¿ç‰¹å¾´é‡ã®ã¿ï¼‰
        n_jobs=-1
    )

    # OOFäºˆæ¸¬
    oof_pred_stacking = cross_val_predict(stacking, X_train, y_train, cv=cv)
    stacking_oof_score = accuracy_score(y_train, oof_pred_stacking)

    # CVè©•ä¾¡
    cv_scores = cross_val_score(stacking, X_train, y_train, cv=cv, scoring='accuracy')

    result = {
        'meta_model': meta_name,
        'oof_score': stacking_oof_score,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std()
    }
    stacking_results.append(result)

    print(f'  OOF Score: {stacking_oof_score:.4f}')
    print(f'  CV Score: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})')
    print(f'  vs Hard Voting: {stacking_oof_score - hard_oof_score:+.4f}')

# ========================================================================
# Stacking with Passthroughï¼ˆå…ƒã®ç‰¹å¾´é‡ã‚‚ä½¿ç”¨ï¼‰
# ========================================================================
print('\n' + '='*80)
print('Stacking with Passthroughï¼ˆå…ƒç‰¹å¾´é‡ + ãƒ¡ã‚¿ç‰¹å¾´é‡ï¼‰')
print('='*80)

stacking_passthrough = StackingClassifier(
    estimators=[
        ('lgb', base_models['LightGBM']),
        ('gb', base_models['GradientBoosting']),
        ('rf', base_models['RandomForest'])
    ],
    final_estimator=LogisticRegression(C=0.1, random_state=42, max_iter=1000),
    cv=5,
    passthrough=True,  # å…ƒã®ç‰¹å¾´é‡ã‚‚ä½¿ã†
    n_jobs=-1
)

oof_pred_passthrough = cross_val_predict(stacking_passthrough, X_train, y_train, cv=cv)
passthrough_oof_score = accuracy_score(y_train, oof_pred_passthrough)

cv_scores_passthrough = cross_val_score(stacking_passthrough, X_train, y_train, cv=cv, scoring='accuracy')

print(f'\nOOF Score: {passthrough_oof_score:.4f}')
print(f'CV Score: {cv_scores_passthrough.mean():.4f} (Â±{cv_scores_passthrough.std():.4f})')
print(f'vs Hard Voting: {passthrough_oof_score - hard_oof_score:+.4f}')

# ========================================================================
# çµæœã‚µãƒãƒªãƒ¼
# ========================================================================
print('\n' + '='*80)
print('çµæœã‚µãƒãƒªãƒ¼')
print('='*80)

all_results = pd.DataFrame([
    {'Model': 'Hard Voting (Baseline)', 'OOF Score': hard_oof_score, 'Improvement': 0.0},
    {'Model': 'Stacking (C=0.1)', 'OOF Score': stacking_results[0]['oof_score'],
     'Improvement': stacking_results[0]['oof_score'] - hard_oof_score},
    {'Model': 'Stacking (C=0.5)', 'OOF Score': stacking_results[1]['oof_score'],
     'Improvement': stacking_results[1]['oof_score'] - hard_oof_score},
    {'Model': 'Stacking (C=1.0)', 'OOF Score': stacking_results[2]['oof_score'],
     'Improvement': stacking_results[2]['oof_score'] - hard_oof_score},
    {'Model': 'Stacking + Passthrough', 'OOF Score': passthrough_oof_score,
     'Improvement': passthrough_oof_score - hard_oof_score}
]).sort_values('OOF Score', ascending=False)

print('\nã€OOF Score Rankingã€‘')
print(all_results.to_string(index=False))

best_model = all_results.iloc[0]
print(f'\nğŸ† Best Model: {best_model["Model"]}')
print(f'   OOF Score: {best_model["OOF Score"]:.4f}')
print(f'   Improvement: {best_model["Improvement"]:+.4f} ({best_model["Improvement"]/hard_oof_score*100:+.2f}%)')

# ç›®æ¨™é”æˆç¢ºèª
target_oof = 0.840
if best_model['OOF Score'] >= target_oof:
    print(f'\nâœ… ç›®æ¨™é”æˆï¼ OOF {target_oof:.3f}ä»¥ä¸Šé”æˆ')
else:
    print(f'\nâš ï¸  ç›®æ¨™æœªé”: OOF {best_model["OOF Score"]:.4f} < ç›®æ¨™ {target_oof:.3f}')
    print(f'   å·®åˆ†: {target_oof - best_model["OOF Score"]:.4f}')

# ========================================================================
# æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã§æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
# ========================================================================
print('\n' + '='*80)
print('æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ')
print('='*80)

# æœ€è‰¯ã®Stackingãƒ¢ãƒ‡ãƒ«ã‚’å†è¨“ç·´
if best_model['Model'] == 'Hard Voting (Baseline)':
    # Hard VotingãŒæœ€è‰¯ã®å ´åˆ
    final_stacking = voting_hard
    print('æœ€è‰¯ãƒ¢ãƒ‡ãƒ«: Hard Votingï¼ˆStackingã‚ˆã‚Šå„ªç§€ï¼‰')
elif best_model['Model'] == 'Stacking + Passthrough':
    final_stacking = stacking_passthrough
else:
    # ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã®Cå€¤ã‚’ç‰¹å®š
    if 'C=' in best_model['Model']:
        c_value = float(best_model['Model'].split('C=')[1].rstrip(')'))
    else:
        c_value = 0.1
    final_stacking = StackingClassifier(
        estimators=[
            ('lgb', base_models['LightGBM']),
            ('gb', base_models['GradientBoosting']),
            ('rf', base_models['RandomForest'])
        ],
        final_estimator=LogisticRegression(C=c_value, random_state=42, max_iter=1000),
        cv=5,
        passthrough=False,
        n_jobs=-1
    )

print('\nè¨“ç·´ä¸­...')
final_stacking.fit(X_train, y_train)

# äºˆæ¸¬
predictions = final_stacking.predict(X_test)

# æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
submission = pd.DataFrame({
    'PassengerId': test['PassengerId'],
    'Survived': predictions
})

filename = 'submission_stacking_ensemble.csv'
submission.to_csv(filename, index=False)

survival_rate = predictions.mean()
print(f'\nâœ… {filename}')
print(f'   Model: {best_model["Model"]}')
print(f'   OOF Score: {best_model["OOF Score"]:.4f}')
print(f'   Survival Rate: {survival_rate:.3f}')

# ========================================================================
# æ¯”è¼ƒç”¨: Hard Votingã‚‚ç”Ÿæˆ
# ========================================================================
print('\næ¯”è¼ƒç”¨: Hard Votingæå‡ºãƒ•ã‚¡ã‚¤ãƒ«')
voting_hard.fit(X_train, y_train)
voting_pred = voting_hard.predict(X_test)

submission_voting = pd.DataFrame({
    'PassengerId': test['PassengerId'],
    'Survived': voting_pred
})

filename_voting = 'submission_hard_voting_baseline.csv'
submission_voting.to_csv(filename_voting, index=False)

print(f'âœ… {filename_voting}')
print(f'   OOF Score: {hard_oof_score:.4f}')
print(f'   Survival Rate: {voting_pred.mean():.3f}')

# çµæœã‚’CSVã«ä¿å­˜
results_df = all_results.copy()
results_df.to_csv('experiments/results/stacking_ensemble_results.csv', index=False)
print(f'\nâœ… çµæœã‚’ä¿å­˜: experiments/results/stacking_ensemble_results.csv')

print('\n' + '='*80)
print('STACKING ENSEMBLEå®Œäº†ï¼')
print('='*80)

print('\næ¨å¥¨æå‡ºé †åº:')
print(f'1. {filename} (Stacking - æœ€è‰¯OOF)')
print(f'2. {filename_voting} (Hard Voting - ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³)')
print('\nKaggleã§æ¤œè¨¼ã—ã¦ãã ã•ã„ï¼')
